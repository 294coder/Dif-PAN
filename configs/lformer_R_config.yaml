optimizer:
  name: "adamw"
  lr: !!float 1e-3
  weight_decay: !!float 1e-6

lr_scheduler:
  # name: "multi_step"
  # milestones: [100, 1000]
  # gamma: 0.1
  # name: 'identity'

  # name: "plateau"
  # mode: "min"
  # threshold: !!float 5e-6 # l1
  # min_lr: !!float 1e-6
  # patience: 30
  # factor: !!float 0.2
  # cooldown: 10

  name: 'cos_anneal_restart'
  T_0: 50
  T_mult: 2
  eta_min: !!float 1e-6

max_norm: 0.03
ema_decay: 0.999

path:
  wv3_train_path: "/Data2/ZiHanCao/datasets/pansharpening/wv3/training_wv3/train_wv3.h5"
  wv3_val_path: "/Data2/ZiHanCao/datasets/pansharpening/wv3/reduced_examples/test_wv3_multiExm1.h5"

  gf2_train_path: "/Data2/ZiHanCao/datasets/pansharpening/gf/training_gf2/train_gf2.h5"
  gf2_val_path: "/Data2/ZiHanCao/datasets/pansharpening/gf/reduced_examples/test_gf2_multiExm1.h5"

network_configs:
  pan_dim: 1
  lms_dim: 4 #102 #128
  attn_dim: 64
  hp_dim: 64
  n_stage: 5
  attn_type: 'R'
  r_op: 'patchembed_v'
  patch_merge: yes
  crop_batch_size: 64
  patch_size_list: [16, 64, 64]
  scale: 4

logger_config:
  base_path: ./log_file/
  name: lformer
  file_mode: w
