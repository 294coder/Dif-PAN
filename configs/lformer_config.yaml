optimizer:
  name: "adamw"
  lr: !!float 1e-3
  weight_decay: !!float 1e-6

lr_scheduler:
  name: "multi_step"
  milestones: [100, 1000]
  gamma: 0.1
  # name: 'identity'

  # name: "plateau"
  # mode: "min"
  # threshold: !!float 5e-6 # l1
  # min_lr: !!float 1e-6
  # patience: 30
  # factor: !!float 0.2
  # cooldown: 10

max_norm: 0.03
ema_decay: 0.999

path:
  wv3_train_path: "/volsparse1/dataset/PanCollection/training_data/train_wv3_9714.h5"
  wv3_val_path: "/volsparse1/dataset/PanCollection/test_data/test_wv3_multiExm1.h5"

  gf2_train_path: "/volsparse1/dataset/PanCollection/training_data/train_gf2_19809.h5"
  gf2_val_path: "/volsparse1/dataset/PanCollection/test_data/test_gf2_multiExm1.h5"

network_configs:
  # lformer_ablation:
  #   pan_dim: 3
  #   lms_dim: 31
  #   attn_dim: 64
  #   hp_dim: 64
  #   n_stage: 5
  lformer:
    pan_dim: 1
    lms_dim: 4 #102 #128
    attn_dim: 16
    hp_dim: 48
    n_stage: 5
    patch_merge: yes
    crop_batch_size: 64
    patch_size_list: [16, 64, 64]
    scale: 4

  lformer_ablation_skip:
    pan_dim: 1
    lms_dim: 8 #102 #128
    attn_dim: 32
    hp_dim: 32
    n_stage: 5
    patch_merge: yes
    crop_batch_size: 32
    patch_size_list: [16, 64, 64]
    scale: 4

logger_config:
  base_path: ./log_file/
  name: lformer
  file_mode: w
