{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 画出预测图像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "from datasets.FLIR_2 import FLIRDataset\n",
    "from datasets.TNO import TNODataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/ZiHanCao/datasets/TNO'\n",
    "ds = TNODataset(path, 'test', no_split=True)\n",
    "dl = th.utils.data.DataLoader(ds, batch_size=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "from model.build_network import build_network\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "torch.cuda.set_device(device)\n",
    "\n",
    "net=build_network('dcformer_mwsa', spectral_num=1, added_c=1, block_list=[4,[4,3],[4,3,2]],mode='C')\n",
    "net.load_state_dict(\n",
    "    # th.load('/home/ZiHanCao/exps/panformer/weight/dcformer_379zkf3e/ep_550.pth', map_location=device)['model']  # 2n8eo45b\n",
    "    # th.load('./weight/dcformer_17rgbfmz/ep_490.pth', map_location=device)['model']\n",
    "    th.load('./weight/dcformer_3l0px5zm/ep_260.pth', map_location=device)['model']\n",
    ")\n",
    "net=net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import savemat\n",
    "import cv2\n",
    "\n",
    "from model.base_model import PatchMergeModule\n",
    "\n",
    "def convert_uint8(img):\n",
    "    print(img.shape, end=' ')\n",
    "    if img.dtype != np.uint8:\n",
    "        img = img.clip(0, 1)\n",
    "        img *= 255\n",
    "        # print('convert to [0, 255]')\n",
    "    return img.astype(np.uint8)\n",
    "\n",
    "net.eval()\n",
    "patch_merge_net = PatchMergeModule(net, 1, patch_size=64, scale=1)\n",
    "with th.no_grad():\n",
    "    for i, (ir, ms, vis, gt) in enumerate(dl):\n",
    "        ir, ms, vis, gt = ir.cuda(), ms.cuda(), vis.cuda(), gt.cuda()\n",
    "        \n",
    "        spa_size = gt.shape[-2:]\n",
    "        pan_nc = ir.size(1)\n",
    "        ms_nc = ms.size(1)\n",
    "        input = (\n",
    "            F.interpolate(ms, size=tuple(vis.shape[-2:]), mode='bilinear', align_corners=True),\n",
    "            vis,\n",
    "            torch.cat([ir, torch.zeros(1, ms_nc - pan_nc, *spa_size).cuda()], dim=1)\n",
    "        )\n",
    "        sr = patch_merge_net.forward_chop(*input)[0]\n",
    "        sr = sr.detach().cpu().numpy()[0]\n",
    "        sr_show = sr.transpose([1, 2, 0])\n",
    "        vis_show = vis.detach().cpu().numpy()[0].transpose([1, 2, 0])\n",
    "        ir_show = ir.detach().cpu().numpy()[0].transpose([1, 2, 0])\n",
    "        \n",
    "        fig, axes = plt.subplots(ncols=3, figsize=(12, 4), dpi=200)\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for img, name, ax in zip([vis_show, ir_show, sr_show],\n",
    "                                 ['vis', 'ir', 'fuse'],\n",
    "                                 axes):\n",
    "            ax.imshow(img, 'gray')\n",
    "            ax.set_axis_off()\n",
    "            ax.set_title(name)\n",
    "        \n",
    "        plt.subplots_adjust(wspace=0, hspace=0)\n",
    "        plt.tight_layout(pad=0)\n",
    "        plt.show()\n",
    "        \n",
    "        sr_show = convert_uint8(sr_show)\n",
    "        cv2.imwrite(f'./visualized_img/ir/{i}.bmp', sr_show)\n",
    "        print('img saved to {}'.format(f'./visualized_img/ir/{i}.bmp'))\n",
    "        \n",
    "        # mat=dict(\n",
    "        #     fuse=convert_uint8(sr_show),\n",
    "        #     vis=convert_uint8(vis_show),\n",
    "        #     ir=convert_uint8(ir_show)\n",
    "        # )\n",
    "        # mat_path = f'./visualized_img/ir/{i}.mat'\n",
    "        \n",
    "        # savemat(mat_path, mat)\n",
    "        # print('mat file saved to {}'.format(mat_path))\n",
    "        \n",
    "        # raw_ir = ir\n",
    "        # y=net.val_step(ms.to('cuda:1'), ir.to('cuda:1'), vis.to('cuda:1'))\n",
    "        # y_show=make_grid(y.detach().cpu(), nrow=4).numpy()\n",
    "        # vis=make_grid(vis, nrow=4).numpy()\n",
    "        # ir=make_grid(ir, nrow=4).numpy()\n",
    "        \n",
    "        # print(th.abs((y.detach().cpu()-raw_ir)).sum())\n",
    "        # print(y_show.min(), y_show.max())\n",
    "        \n",
    "        # ys = np.concatenate([vis, ir, y_show], axis=2)\n",
    "        \n",
    "        # ax=plt.gca()\n",
    "        # ax.set_axis_off()\n",
    "        # plt.gcf().set_dpi(300)\n",
    "        # plt.gcf().set_size_inches(4, 4*3)\n",
    "        # plt.tight_layout(pad=0)\n",
    "        # ax.imshow(ys.transpose(1,2,0))\n",
    "        # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import savemat, loadmat\n",
    "\n",
    "path = r'./visualized_img/ir/data_2.mat'\n",
    "mat_d = loadmat(path)\n",
    "fuse = mat_d['fuse']\n",
    "np.min(fuse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.random.randn(3, 64, 64)\n",
    "y=np.random.randn(3, 64, 64)\n",
    "\n",
    "np.concatenate([x,y], axis=-1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 可视化attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "from model.module.attention import MultiScaleWindowCrossAttention, CAttention\n",
    "from model.module.swin import window_partition\n",
    "from model.module.helper_func import exists\n",
    "from model.dcformer_mwsa import DCFormerMWSA\n",
    "\n",
    "from datasets.HISR import HISRDataSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = DCFormerMWSA(spectral_num=31, added_c=3, block_list=[4,[4,3],[4,3,2]],mode='C')#.cuda(1)\n",
    "net.eval()\n",
    "\n",
    "path = \"/home/ZiHanCao/datasets/HISI/new_cave/test_cave(with_up)x4.h5\"\n",
    "ds = HISRDataSets(h5py.File(path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "\n",
    "CATTEN_LIST = []\n",
    "CATTN_OUT_LIST = []\n",
    "MWSA_ATTN_LIST = []\n",
    "MWSA_ATTN_OUT_LIST = []\n",
    "GHOST_OUT_LIST = []\n",
    "\n",
    "def cattention_hook(m, inp, outp):\n",
    "    # calcu attn again\n",
    "    x = inp[0]\n",
    "    with torch.no_grad():\n",
    "        qkv = m.qkv_dwconv(m.qkv(x))\n",
    "        q, k, v = qkv.chunk(3, dim=1)\n",
    "\n",
    "        q = rearrange(q, \"b (head c) h w -> b head c (h w)\", head=m.num_heads)\n",
    "        k = rearrange(k, \"b (head c) h w -> b head c (h w)\", head=m.num_heads)\n",
    "        v = rearrange(v, \"b (head c) h w -> b head c (h w)\", head=m.num_heads)\n",
    "\n",
    "        q = torch.nn.functional.normalize(q, dim=-1)\n",
    "        k = torch.nn.functional.normalize(k, dim=-1)\n",
    "        attn = (q @ k.transpose(-2, -1)) * m.temperature\n",
    "        attn = attn.softmax(dim=-1)\n",
    "    \n",
    "    CATTEN_LIST.append(attn.detach().cpu().numpy())\n",
    "    CATTN_OUT_LIST.append(outp.detach().cpu().numpy())\n",
    "    \n",
    "def mwsa_attn_hook(m, inp, outp):\n",
    "    tgt, mem = inp\n",
    "    with torch.no_grad():\n",
    "        if not exists(m.window_size1) and not exists(m.window_size2):\n",
    "                m.window_size1 = m.window_dict[tgt.size(-1)]\n",
    "                m.window_size2 = m.window_dict[mem.size(-1)]\n",
    "\n",
    "        b, c, h, w = tgt.shape\n",
    "        mem = m.match_c(mem)\n",
    "        q = window_partition(\n",
    "            tgt.permute(0, 2, 3, 1), m.window_size1\n",
    "        )  # [nw*b, wh1, ww1, c]\n",
    "        kv = window_partition(\n",
    "            mem.permute(0, 2, 3, 1), m.window_size2\n",
    "        )  # [nw*b, wh2, ww2, c]\n",
    "\n",
    "        q = m.q(q)\n",
    "        kv = m.kv(kv)\n",
    "        k, v = kv.chunk(2, dim=-1)\n",
    "\n",
    "        # assert tgt.size(0) == mem.size(0)\n",
    "\n",
    "        # q: [b*nw, nh, wh1*ww1, c]\n",
    "        # k, v: [b*nw, nh, wh2*ww2, c]\n",
    "        q = rearrange(q, \"b h w (head c) -> b head (h w) c\", head=m.num_heads)\n",
    "        k = rearrange(k, \"b h w (head c) -> b head (h w) c\", head=m.num_heads)\n",
    "        v = rearrange(v, \"b h w (head c) -> b head (h w) c\", head=m.num_heads)\n",
    "\n",
    "        q = F.normalize(q, dim=-1)\n",
    "        k = F.normalize(k, dim=-1)\n",
    "\n",
    "        # [b*nw, nh, wh1*ww1, wh2*ww2]\n",
    "        attn = (q @ k.transpose(-2, -1)) * m.temperature\n",
    "\n",
    "        attn = attn.softmax(-1)\n",
    "        ghost_out = m.ghost_module(\n",
    "            torch.cat([tgt, F.interpolate(mem, tgt.shape[-1], mode=\"bilinear\")], dim=1)\n",
    "        )\n",
    "    MWSA_ATTN_LIST.append(attn.detach().cpu().numpy())\n",
    "    MWSA_ATTN_OUT_LIST.append(outp.detach().cpu().numpy())\n",
    "    GHOST_OUT_LIST.append(ghost_out.detach().cpu().numpy())\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_attn_handlers = []\n",
    "mwsa_attn_hooks = []\n",
    "\n",
    "for m in net.modules():\n",
    "    if isinstance(m, CAttention):\n",
    "        h = m.register_forward_hook(cattention_hook)\n",
    "        c_attn_handlers.append(h)\n",
    "        print('c attention hook')\n",
    "    \n",
    "    if isinstance(m, MultiScaleWindowCrossAttention):\n",
    "        h = m.register_forward_hook(mwsa_attn_hook)\n",
    "        mwsa_attn_hooks.append(h)\n",
    "        print('mwsa attention hook')\n",
    "        \n",
    "        \n",
    "def remove_all_hooks(hs):\n",
    "    for h in hs:\n",
    "        h.remove()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_all_hooks(c_attn_handlers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = torch.utils.data.DataLoader(ds, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pan, ms, lms, gt = next(iter(dl))\n",
    "\n",
    "out = net.val_step(ms.float(),#.cuda(1),\n",
    "                   lms.float(),#.cuda(1),\n",
    "                   pan.float())#.cuda(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# norm to [0, 1]\n",
    "def norm(x):\n",
    "    return (x - x.min()) / (x.max() - x.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(25, 10))\n",
    "axes = axes.flatten()\n",
    "for i, c in enumerate(MWSA_ATTN_OUT_LIST):\n",
    "    c = c.mean(1)[0]\n",
    "    # c = c[0, 5]\n",
    "    axes[i].imshow(norm(c), 'hot')\n",
    "    # axes[i].set_axis_off()\n",
    "    axes[i].set_title(f'MWSA Attention {i}')\n",
    "    \n",
    "axes[-1].set_axis_off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MWSA_ATTN_OUT_LIST[0].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sailency map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as th\n",
    "import PIL.Image as pim\n",
    "\n",
    "from sailency import LAM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LAM()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multi-source projection head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.dcformer_mwsa import DCFormerMWSA\n",
    "from model.base_model import BaseModel, register_model\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ResidualConvBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self, channels: int, kernel_size: int, stride: int, padding: int\n",
    "    ) -> None:\n",
    "        super(ResidualConvBlock, self).__init__()\n",
    "        self.rcb = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                channels,\n",
    "                channels,\n",
    "                (kernel_size, kernel_size),\n",
    "                (stride, stride),\n",
    "                (padding, padding),\n",
    "            ),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv2d(\n",
    "                channels,\n",
    "                channels,\n",
    "                (kernel_size, kernel_size),\n",
    "                (stride, stride),\n",
    "                (padding, padding),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.rcb(x)\n",
    "\n",
    "        out = torch.add(out, identity)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "# Reference from `https://github.com/SHI-Labs/Cross-Scale-Non-Local-Attention/blob/master/src/model/utils/tools.py`\n",
    "class MultiSourceProjection(nn.Module):\n",
    "    def __init__(self, channels, kernel_size, scale) -> None:\n",
    "        super(MultiSourceProjection, self).__init__()\n",
    "        if scale == 2:\n",
    "            de_kernel_size = 6\n",
    "            stride = 2\n",
    "            padding = 2\n",
    "            upscale_factor = 2\n",
    "        elif scale == 3:\n",
    "            de_kernel_size = 9\n",
    "            stride = 3\n",
    "            padding = 3\n",
    "            upscale_factor = 3\n",
    "        elif scale == 4:\n",
    "            de_kernel_size = 12\n",
    "            stride = 4\n",
    "            padding = 4\n",
    "\n",
    "        self.down_conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                channels,\n",
    "                channels,\n",
    "                (de_kernel_size, de_kernel_size),\n",
    "                (stride, stride),\n",
    "                (padding, padding),\n",
    "            ),\n",
    "            nn.PReLU(),\n",
    "        )\n",
    "        self.diff_encode1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                channels,\n",
    "                channels,\n",
    "                (de_kernel_size, de_kernel_size),\n",
    "                (stride, stride),\n",
    "                (padding, padding),\n",
    "            ),\n",
    "            nn.PReLU(),\n",
    "        )\n",
    "\n",
    "        self.encoder = ResidualConvBlock(channels, kernel_size, 1, kernel_size // 2)\n",
    "\n",
    "    def forward(self, x, x_is: torch.Tensor, x_cs: torch.Tensor) -> torch.Tensor:\n",
    "        # cross_scale_attention = self.cross_scale_attention(x)\n",
    "        # non_local_attention = self.non_local_attention(x)\n",
    "\n",
    "        diff = self.encoder(x_cs - x_is)\n",
    "        out = x_is + diff\n",
    "\n",
    "        down_conv1 = self.down_conv1(out)\n",
    "        diff1 = torch.sub(x, down_conv1)\n",
    "        diff_encode1 = self.diff_encode1(diff1)\n",
    "        estimate = torch.add(out, diff_encode1)\n",
    "        # down_conv1 = self.down_conv1(multi_source_projection1)\n",
    "        # diff1 = torch.sub(x, down_conv1)\n",
    "        # diff_encode1 = self.diff_encode1(diff1)\n",
    "        # estimate = torch.add(multi_source_projection1, diff_encode1)\n",
    "        return estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msp = MultiSourceProjection(3, 3, 4)\n",
    "\n",
    "x = torch.randn(1, 3, 16, 16)\n",
    "in_x = torch.randn(1, 3, 64, 64)\n",
    "cs_x = torch.randn(1, 3, 64, 64)\n",
    "\n",
    "print(msp(x, in_x, cs_x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "path = \"/media/office-401-remote/Elements SE/cao/ZiHanCao/datasets/HISI/new_cave/test_cave(with_up)x4.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 31, 512, 512)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file=h5py.File(path)\n",
    "gt=file['GT'][:]\n",
    "gt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sio.savemat('gt.mat', {'gt': gt})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "ed7adef37e3f9076e8fefe1da9ab276f7af66cb3d176017da49fc0fb514638e4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
